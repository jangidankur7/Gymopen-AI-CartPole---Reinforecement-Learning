{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer games is an example for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RL : Hit and Trial Method\n",
    "- Just like learning to ride a cycle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Its parts are :\n",
    "- ***AI Bot*** which intracts with the environment\n",
    "- Environment with a state\n",
    "- Agent interacts with environment with some action \n",
    "- That action will belong to a set of actions\n",
    "- Now, environment will return a state fromm the set of states to the agent\n",
    "- Environment will also giive some reward to agent along with state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching our AI bot:\n",
    "- We will use Gym openAI\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loaded environment (game with name CartPole)\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMES WITH certain import methods :\n",
    "- action_space\n",
    "- observation_space\n",
    "- reset() : Returns init state and also resets the env\n",
    "- step()\n",
    "- render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0279591 , 0.01030987, 0.01892451, 0.00896347])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() ## Take game to initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space ## Means we can move only right or left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n ## 2 actions we can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "    random_action = env.action_space.sample()\n",
    "    env.step(random_action) ## randomly move lt or rt\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Playing CartPole with random statergy :\n",
    "- Game episode : We are going to play multiple games with different time duration\n",
    "- Step() function in more detail\n",
    "- game over ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/20 High Score :33\n",
      "Game Episode :1/20 High Score :27\n",
      "Game Episode :2/20 High Score :43\n",
      "Game Episode :3/20 High Score :32\n",
      "Game Episode :4/20 High Score :39\n",
      "Game Episode :5/20 High Score :12\n",
      "Game Episode :6/20 High Score :18\n",
      "Game Episode :7/20 High Score :12\n",
      "Game Episode :8/20 High Score :12\n",
      "Game Episode :9/20 High Score :18\n",
      "Game Episode :10/20 High Score :15\n",
      "Game Episode :11/20 High Score :25\n",
      "Game Episode :12/20 High Score :20\n",
      "Game Episode :13/20 High Score :11\n",
      "Game Episode :14/20 High Score :31\n",
      "Game Episode :15/20 High Score :37\n",
      "Game Episode :16/20 High Score :13\n",
      "Game Episode :17/20 High Score :20\n",
      "Game Episode :18/20 High Score :10\n",
      "Game Episode :19/20 High Score :16\n",
      "All 20 episodes are over\n"
     ]
    }
   ],
   "source": [
    "for e in range(20): ## we will play 20 games \n",
    "    #e = episode\n",
    "    observation = env.reset()\n",
    "    \n",
    "    for t in range(50):\n",
    "         # 50 timestep is maximum time for which we will play one episode or game\n",
    "            \n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation,reward,done,other_info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            print(\"Game Episode :{}/{} High Score :{}\".format(e,20,t))\n",
    "            break\n",
    "            \n",
    "env.close()\n",
    "print(\"All 20 episodes are over\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q - Learning :\n",
    "- Building our own statergy\n",
    "- Q function : Q(s,a)\n",
    "- Q(s,a) where 's' is state and 'a' is action to take corrosponding to that state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation:\n",
    "'''https://ai.stackexchange.com/questions/11057/what-is-the-bellman-operator-in-reinforcement-learning'''\n",
    "\n",
    "- Q(s,a) = r + (gamma) * MAX{Q(S_,a_)}\n",
    "- gamma = Discount Factor\n",
    "- r = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Design Exploration VS Exploitation Tradeoff\n",
    "- Exploration is good in begening : It helps us to try various random things\n",
    "- Exploitation is good at the end : It helps to sample good experience from the past(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95 ## Discount factor\n",
    "        self.epsilon = 1.0 ## 100% Random exploration in the begning\n",
    "        self.epsilon_decay = 0.995 ## It will trust the knowledge 0.05% from past experience and learn by exploration rest 99.5%\\\n",
    "        self.epsilon_min = 0.01 ##{Work at last when we gained full knowledge by playing lot of games}\n",
    "        ##Even if I have played 1000 games, I will take 1% random action and rest will be from past knowwledge\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._createmodel()\n",
    "        \n",
    "    def _createmodel(self):   \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu'))\n",
    "\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "\n",
    "        model.compile(loss='mse',optimizer=Adam(lr = 0.001))\n",
    "\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        # Remember all past experiences\n",
    "        self.memory.append(state,action,reward,next_state,done)\n",
    "    \n",
    "    def act(self,state):\n",
    "        # Epsilon greedy method\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            ## In this case, take a random action \n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        ## In else case, we will ask NN to give a suitable action\n",
    "        \n",
    "        return np.argmax(model.predict(state)[0])\n",
    "    \n",
    "    \n",
    "    def train(self,batch_size):\n",
    "        # Train using Replay Buffer\n",
    "        \n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        \n",
    "        for experience in minibatch:\n",
    "            state,action,reward,next_state,done = experience\n",
    "            \n",
    "            ## X : State\n",
    "            ## Y : Expected Reward\n",
    "            \n",
    "            if not done:\n",
    "                # Gameis not yet over, Bellman eqn to approx the target_value of reward\n",
    "                \n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "                \n",
    "            else:\n",
    "\n",
    "                target = reawrd\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            # X = state\n",
    "            # Y = target_f\n",
    "\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "                \n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                \n",
    "                self.epsilon *= self.epsilon_decay\n",
    "                \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "        \n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why agent needs a memory ?\n",
    "- You dont havve any training data\n",
    "- We will use past experiences of agent in memory to play the game \n",
    "- We will use that memory to train neural network \n",
    "\n",
    "- We will use double endded queue where we can add or remove items from both ends\n",
    "- We will remove old experience and add new experience to that memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Example of what model we are using\n",
    "model = Sequential()\n",
    "model.add(Dense(24,input_dim=4,activation='relu'))\n",
    "\n",
    "model.add(Dense(24,activation='relu'))\n",
    "\n",
    "model.add(Dense(2,activation='linear'))\n",
    "\n",
    "model.compile(loss='mse',optimizer=Adam(lr = 0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22452378,  0.19204345]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(1,4)\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the DQN Agent (Deep Q-Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "output_dir = \"carpole_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=4,action_size=2)\n",
    "done = False\n",
    "state_size = 4\n",
    "action_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "append() takes exactly one argument (5 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0ecc532ee67a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m  \u001b[1;31m########## IMPORTANT #########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## Experience for the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-f38447462a40>\u001b[0m in \u001b[0;36mremember\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Remember all past experiences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: append() takes exactly one argument (5 given)"
     ]
    }
   ],
   "source": [
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        \n",
    "        env.render()\n",
    "        action = agent.act(state) ## Action is 0 or 1\n",
    "        next_state,reward,done,other_info = env.step(action)\n",
    "        reward = reward if not done else -10  ########## IMPORTANT #########\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done) ## Experience for the agent\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            \n",
    "            print(\"Game Episode :{}/{} High Score :{} Exploration Rate {:.2}\".format(e,20,t,agent.epsilon))\n",
    "            break\n",
    "            \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.train(batch_size)\n",
    "        \n",
    "    if e%50 == 0:\n",
    "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
    "\n",
    "print(\"Deep Q-Learner Model trained\")\n",
    "env.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
